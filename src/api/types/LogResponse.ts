/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Humanloop from "../index";

/**
 * Request model for logging a datapoint.
 */
export interface LogResponse {
    /** The name of the project associated with this log */
    project?: string;
    /** The unique ID of the project associated with this log. */
    projectId?: string;
    /** ID of the session to associate the datapoint. */
    sessionId?: string;
    /** A unique string identifying the session to associate the datapoint to. Allows you to log multiple datapoints to a session (using an ID kept by your internal systems) by passing the same `session_reference_id` in subsequent log requests. Specify at most one of this or `session_id`. */
    sessionReferenceId?: string;
    /** ID associated to the parent datapoint in a session. */
    parentId?: string;
    /** A unique string identifying the previously-logged parent datapoint in a session. Allows you to log nested datapoints with your internal system IDs by passing the same reference ID as `parent_id` in a prior log request. Specify at most one of this or `parent_id`. Note that this cannot refer to a datapoint being logged in the same request. */
    parentReferenceId?: string;
    /** The inputs passed to the prompt template. */
    inputs?: Record<string, unknown>;
    /** Identifies where the model was called from. */
    source?: string;
    /** Any additional metadata to record. */
    metadata?: Record<string, unknown>;
    /** Whether the request/response payloads will be stored on Humanloop. */
    save?: boolean;
    /** ID of the source datapoint if this is a log derived from a datapoint in a dataset. */
    sourceDatapointId?: string;
    /** String ID of logged datapoint. Starts with `data_`. */
    id: string;
    /** Unique user-provided string identifying the datapoint. */
    referenceId?: string;
    /** Unique ID of an experiment trial to associate to the log. */
    trialId?: string;
    /** The messages passed to the to provider chat endpoint. */
    messages?: Humanloop.ChatMessageWithToolCall[];
    /** Generated output from your model for the provided inputs. Can be `None` if logging an error, or if logging a parent datapoint with the intention to populate it later */
    output?: string;
    judgment?: Humanloop.LogResponseJudgment;
    /** Unique ID of a config to associate to the log. */
    configId?: string;
    config: Humanloop.ConfigResponse;
    /** The environment name used to create the log. */
    environment?: string;
    feedback?: Humanloop.FeedbackResponse[];
    /** User defined timestamp for when the log was created. */
    createdAt?: Date;
    /** Error message if the log is an error. */
    error?: string;
    /** Duration of the logged event in seconds. */
    duration?: number;
    /** The message returned by the provider. */
    outputMessage?: Humanloop.ChatMessageWithToolCall;
    /** Number of tokens in the prompt used to generate the output. */
    promptTokens?: number;
    /** Number of tokens in the output generated by the model. */
    outputTokens?: number;
    /** Cost in dollars associated to the tokens in the prompt. */
    promptCost?: number;
    /** Cost in dollars associated to the tokens in the output. */
    outputCost?: number;
    /** Raw request sent to provider. */
    providerRequest?: Record<string, unknown>;
    /** Raw response received the provider. */
    providerResponse?: Record<string, unknown>;
    /** User email address provided when creating the datapoint. */
    user?: string;
    /** Latency of provider response. */
    providerLatency?: number;
    /** Total number of tokens in the prompt and output. */
    tokens?: number;
    /** Raw output from the provider. */
    rawOutput?: string;
    /** Reason the generation finished. */
    finishReason?: string;
    metricValues?: Humanloop.MetricValueResponse[];
    tools?: Humanloop.ToolResultResponse[];
    /** Controls how the model uses tools. The following options are supported: 'none' forces the model to not call a tool; the default when no tools are provided as part of the model config. 'auto' the model can decide to call one of the provided tools; the default when tools are provided as part of the model config. Providing {'type': 'function', 'function': {name': <TOOL_NAME>}} forces the model to use the named function. */
    toolChoice?: Humanloop.LogResponseToolChoice;
    evaluationResults: Humanloop.EvaluationResultResponse[];
    observabilityStatus: Humanloop.ObservabilityStatus;
    updatedAt: Date;
    /** List of batch IDs the log belongs to. */
    batchIds?: string[];
}
