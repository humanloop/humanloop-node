/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Humanloop from "../index";

/**
 * Request model for creating an Evaluation.
 *
 * Evaluation benchmark your Prompt/Tool Versions. With the Datapoints in a Dataset Version,
 * Logs corresponding to the Datapoint and each Evaluated Version are evaluated by the specified Evaluator Versions.
 * Aggregated statistics are then calculated and presented in the Evaluation.
 */
export interface CreateEvaluationRequest {
    /** The Dataset Version to use in this Evaluation. */
    dataset: Humanloop.EvaluationsDatasetRequest;
    /** Unique identifiers for the Prompt/Tool Versions to include in the Evaluation Report. Can be left unpopulated if you wish to add evaluatees to this Evaluation Report by specifying `evaluation_id` in Log calls. */
    evaluatees?: Humanloop.EvaluateeRequest[];
    /** The Evaluators used to evaluate. */
    evaluators: Humanloop.EvaluationsRequest[];
}
