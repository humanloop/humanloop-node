/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Humanloop from "../../../../index";

/**
 * @example
 *     {
 *         path: "Banking/Teller Agent",
 *         provider: "anthropic",
 *         endpoint: "chat",
 *         model: "claude-3-7-sonnet-latest",
 *         reasoningEffort: 1024,
 *         template: [{
 *                 role: "system",
 *                 content: "You are a helpful digital assistant, helping users navigate our digital banking platform."
 *             }],
 *         maxIterations: 3,
 *         tools: [{
 *                 type: "inline",
 *                 jsonSchema: {
 *                     name: "stop",
 *                     description: "Call this tool when you have finished your task.",
 *                     parameters: {
 *                         "type": "object",
 *                         "properties": {
 *                             "output": {
 *                                 "type": "string",
 *                                 "description": "The final output to return to the user."
 *                             }
 *                         },
 *                         "additionalProperties": false,
 *                         "required": [
 *                             "output"
 *                         ]
 *                     },
 *                     strict: true
 *                 },
 *                 onAgentCall: "stop"
 *             }],
 *         versionName: "teller-agent-v1",
 *         versionDescription: "Initial version"
 *     }
 */
export interface AgentRequest {
    /** Path of the Agent, including the name. This locates the Agent in the Humanloop filesystem and is used as as a unique identifier. For example: `folder/name` or just `name`. */
    path?: string;
    /** ID for an existing Agent. */
    id?: string;
    /** The model instance used, e.g. `gpt-4`. See [supported models](https://humanloop.com/docs/reference/supported-models) */
    model: string;
    /** The provider model endpoint used. */
    endpoint?: Humanloop.ModelEndpoints;
    /**
     * The template contains the main structure and instructions for the model, including input variables for dynamic values.
     *
     * For chat models, provide the template as a ChatTemplate (a list of messages), e.g. a system message, followed by a user message with an input variable.
     * For completion models, provide a prompt template as a string.
     *
     * Input variables should be specified with double curly bracket syntax: `{{input_name}}`.
     */
    template?: Humanloop.AgentRequestTemplate;
    /** The template language to use for rendering the template. */
    templateLanguage?: Humanloop.TemplateLanguage;
    /** The company providing the underlying model service. */
    provider?: Humanloop.ModelProviders;
    /** The maximum number of tokens to generate. Provide max_tokens=-1 to dynamically calculate the maximum number of tokens to generate given the length of the prompt */
    maxTokens?: number;
    /** What sampling temperature to use when making a generation. Higher values means the model will be more creative. */
    temperature?: number;
    /** An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. */
    topP?: number;
    /** The string (or list of strings) after which the model will stop generating. The returned text will not contain the stop sequence. */
    stop?: Humanloop.AgentRequestStop;
    /** Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the generation so far. */
    presencePenalty?: number;
    /** Number between -2.0 and 2.0. Positive values penalize new tokens based on how frequently they appear in the generation so far. */
    frequencyPenalty?: number;
    /** Other parameter values to be passed to the provider call. */
    other?: Record<string, unknown>;
    /** If specified, model will make a best effort to sample deterministically, but it is not guaranteed. */
    seed?: number;
    /** The format of the response. Only `{"type": "json_object"}` is currently supported for chat. */
    responseFormat?: Humanloop.ResponseFormat;
    /** Guidance on how many reasoning tokens it should generate before creating a response to the prompt. OpenAI reasoning models (o1, o3-mini) expect a OpenAIReasoningEffort enum. Anthropic reasoning models expect an integer, which signifies the maximum token budget. */
    reasoningEffort?: Humanloop.AgentRequestReasoningEffort;
    tools?: Humanloop.AgentRequestToolsItem[];
    /** Additional fields to describe the Prompt. Helpful to separate Prompt versions from each other with details on how they were created or used. */
    attributes?: Record<string, unknown>;
    /** The maximum number of iterations the Agent can run. This is used to limit the number of times the Agent model is called. */
    maxIterations?: number;
    /** Unique name for the Prompt version. Each Prompt can only have one version with a given name. */
    versionName?: string;
    /** Description of the Version. */
    versionDescription?: string;
    /** Description of the Prompt. */
    description?: string;
    /** List of tags associated with this prompt. */
    tags?: string[];
    /** Long description of the Prompt. */
    readme?: string;
}
